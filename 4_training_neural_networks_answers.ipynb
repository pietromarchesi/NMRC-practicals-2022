{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural networks\n",
    "\n",
    "In this tutorial, we are going to explore how we can train an artificial neural networks with backpropagation. Crucially, this allows us to train networks with _multiple layers_. In the first part, you will have to derive expressions to update the weights of the network across layers, and in the second part we will experiment with a modern neural network to classify handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In tutorial 2, we showed how a single neuron cannot compute the XOR function (or any nonlinear function for that matter), but how the problem can be solved if we harvest the computational power of a network of neurons. In assignment 3, we went on to see how we could develop an algorithm that allows us to train the parameter of a single neuron, and we concluded by presenting another version of a nonlinear problem that cannot be solved by a linear decision boundary (the red and green 'moons'). In this introduction, we start off with the same moons dataset and show how a two layer network can learn the nonlinear relation in the data.\n",
    "\n",
    "In the cell below, we train a neural network with two layers on the moon dataset from tutorial 3. If you run it, two plots will appear. On the left is a plot of just the dataset (we only generate a few more samples this time). On the right, we plot the data again, and we show the decision boundary by shading the areas of input space that the network classifies as the 'green' class and as the 'red' class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "X, y = make_moons(n_samples=4000, shuffle=True, noise=0.05)\n",
    "split = 2000\n",
    "x_train, y_train = X[:split], y[:split]\n",
    "x_test, y_test   = X[split:], y[split:]\n",
    "\n",
    "batch_size = 100\n",
    "num_classes = 2\n",
    "n_units = [100, 100]\n",
    "n_epochs = 30\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train\n",
    "x_test \n",
    "\n",
    "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(n_units[0], activation='relu', input_shape=(2,)))\n",
    "\n",
    "if len(n_units) > 1:\n",
    "    for nu in n_units[1:]:\n",
    "        model.add(Dense(nu, activation='relu'))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train_onehot,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_train, y_train_onehot))\n",
    "\n",
    "score = model.evaluate(x_test, y_test_onehot, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# plotting\n",
    "x_min, x_max, y_min, y_max = -1.5, 2.5, -1, 1.5\n",
    "h = 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "plot_pred = model.predict(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "Z = plot_pred.reshape(xx.shape)\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=[15, 8])\n",
    "ax[0].scatter(x_train[y_train == 0, 0], x_train[y_train == 0, 1], color='g')\n",
    "ax[0].scatter(x_train[y_train == 1, 0], x_train[y_train == 1, 1], color='r')\n",
    "ax[0].set_xlabel('x1')\n",
    "ax[0].set_ylabel('x2')\n",
    "\n",
    "ax[1].contourf(xx, yy, Z, cmap='RdYlGn', alpha=.8)\n",
    "ax[1].scatter(x_train[y_train == 0, 0], x_train[y_train == 0, 1], color='g')\n",
    "ax[1].scatter(x_train[y_train == 1, 0], x_train[y_train == 1, 1], color='r')\n",
    "ax[1].set_xlabel('x1')\n",
    "ax[1].set_ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the network can generate a nonlinear decision boundary that carves out the shape of the moons, so as to correctly classify the data! You may notice a somewhat graded transition between green and red: that is because our network is really giving us a _probability_ that a sample belongs to a certain class, and it is up to us to turn that probability into a decision. We will come back to a cool application of neural networks in part 2, but now let's dig deeper into how backpropagation actually works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Understanding backpropagation\n",
    "\n",
    "We begin with the simplest possible 'deep' neural network, composed of a single-unit hidden layer and one output layer. This network is not going to be useful for anything, but it will allow us to illustrate how we can perform backpropagation on deep (i.e. more than one layer) architectures. \n",
    "\n",
    "![title](images/bp_1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, a neural network can be seen as a chain of computation. We begin with our input $x$, then at every layer we compute the weighted sum of the input, pass it through a nonlinearity $f$ (the transfer function) and compute the output. The output of the final layer, together with the target output, will allow us to compute a loss. We can then use this loss to update our parameters using backpropagation. For this tutorial, we will use\n",
    "\n",
    "\\begin{align}\n",
    "L &= \\frac{1}{2}(\\hat{y} - y)^2 \\\\\n",
    "f(x) &= \\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{align}\n",
    "\n",
    "We will also not explicitly worry about the bias, to keep things a little bit simpler, so consider $b_1 =b_2=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Write the output $\\hat{y}$ as a function of $x$, $w_1$, and $w_2$. Write the transfer function simply as $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "\\begin{align}\n",
    "\\hat{y} &= f_2(s_2)   \\\\  \n",
    "&s_2 = w_2z_1 \\\\\\\\\n",
    "&\\hat{y}= f_2(w_2z_1)\\\\\n",
    "&z_1=f_1(s_1)\\\\\\\\\n",
    "&\\hat{y}= f_2(w_2 \\cdot f_1(s_1))\\\\\n",
    "& s_1 = w_1 x \\\\\\\\\n",
    "&\\hat{y}= f(w_2 \\cdot f(w_1 x)) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Suppose that \n",
    "\\begin{align}\n",
    "x &= 1.5 \\\\\n",
    "w_1 &= 0.5 \\\\\n",
    "w_2 &= 0.3 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Use the equation you derived to compute the output $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2\n",
    "fill in the function:<br>\n",
    "\\begin{align}\n",
    "\\hat{y} = f_2(w_2 \\cdot f_1(w_1 x))\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer 2 computed\n",
    "import math\n",
    "x = 1.5\n",
    "w1 = 0.5\n",
    "w2 = 0.3\n",
    "\n",
    "z1 = 1/(1+math.exp(-x*w1))\n",
    "print(z1)\n",
    "\n",
    "yhat = 1/(1+math.exp(-z1*w2))\n",
    "print(yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how our network produces output given some input. This is referred to as a _forward pass_, where we go from input to output. Now, supposing we have compared our output with a target value, and know the loss, we need to 'propagate' this loss backward to update the parameters (most importantly the weights) which generated the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Derive an expression for $\\frac{\\partial L}{\\partial w_2}$.\n",
    "\n",
    "_Hint:_ Expand the derivative using the __chain rule__ as you saw in the lecture! You should arrive to this formula: $(\\hat{y} - y) \\sigma(w_2 z_1)(1- \\sigma(w_2 z_1)) z_1$ (to arrive at it, you will have to fill in the sigmoid function $\\sigma(x)$, compute its derivative, and write that derivative as a function of $\\sigma(x)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3\n",
    "In order to apply the chain rule, realize that $L$ is dependend on $\\hat{y}$, which in turn is dependend on $s_2$, which finally is dependend on $w_2$. Following this reasoning we get:\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_2} &= \n",
    "\\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial w_2}=\n",
    "\\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial s_2}\n",
    "\\frac{\\partial s_2}{\\partial w_2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Then the derivative of the sigmoid function ($f=\\sigma(x)$) is given by:\n",
    "\\begin{align}\n",
    "\\sigma(x)' = \\sigma(x)(1-\\sigma(x))\n",
    "\\end{align}\n",
    "\n",
    "From the formulas in Question 1 we can get the partial derivatives:\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} &= [\\frac{1}{2}(\\hat{y}^2 - 2\\hat{y}y+y^2]' = (\\hat{y} - y) \\\\\n",
    "\\frac{\\partial \\hat{y}}{\\partial s_2} &= [\\sigma (s_2)]' = \n",
    "\\sigma (s_2)(1 - \\sigma (s_2)) = \\sigma (w_2z_1)(1 - \\sigma (w_2 z_1))\\\\\n",
    "\\frac{\\partial s_2}{\\partial w_2} &= z_1\n",
    "\\end{align}\n",
    "\n",
    "Resulting in:\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_2} = (\\hat{y} - y)\\sigma (w_2z_1)(1 - \\sigma (w_2 z_1))z_1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Derive an expression for $\\frac{\\partial L}{\\partial w_1}$.\n",
    "\n",
    "_Hint:_ This is going to be just a longer chain rule expansion!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4\n",
    "Same thing as in answer 3, but instead of expanding $\\frac{\\partial \\hat{y}}{\\partial s_2}$ toward $ w_2$, expand it to $z_1$ and keep going untill you are at $w_1$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_2} &= \\frac{\\partial L}{\\partial \\hat{y}}  \\frac{\\partial \\hat{y}}{\\partial s_2} \\frac{\\partial s_2}{\\partial z_1} \\frac{\\partial z_1}{\\partial s_1} \\frac{\\partial s_1}{\\partial w_1} \\\\\n",
    "&=  (\\hat{y} - y) \\sigma(w_2 z_1)  (1- \\sigma(w_2 z_1)) w_2 \\sigma(w_1 x)(1- \\sigma(w_1 x)) x\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chances are computing these derivatives was a little confusing, so let's not lose track of the point that we are trying to make here:\n",
    "\n",
    "- Our neural network is essentially a series of chained computations. At every layer, we compute something based on the input we received and feed it to the next layer.\n",
    "- We only know the loss at the very end of the chain: we know what the output neurons should output, but no one is telling us what the hidden layers should be doing.\n",
    "- With a smart use of the chain rule, we can figure out how we should change the parameters _at every step in the chain_ such that we get a lower loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Suppose $w_2$ = 0.3, we have a learning rate $\\alpha=0.1$, and we computed $\\frac{\\partial L}{\\partial w_2}$ = 0.8. Update $w_2$ using gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 5\n",
    "\\begin{equation}\n",
    "w_2 = w_2 - \\alpha \\frac{\\partial L}{\\partial w_2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer 5 computed\n",
    "w2 = 0.3\n",
    "alpha = 0.1\n",
    "dldw2 = 0.8\n",
    "\n",
    "neww2 = w2 - alpha*dldw2\n",
    "print(neww2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Neural networks in action\n",
    "\n",
    " The networks we have seen so far were nothing more than toy examples, created to illustrate some important features of neural networks, and they don't really do anything cool. But our brain is capable of incredibly complex computations, and so are (with some limitations, of course) artificial neural networks. In fact, the successes of artificial intelligence (which often means really fancy neural networks) are everywhere on the media. Having stacked layers like the one in our example, where one layer feeds into the next ('deep' architectures) and being able to train the parameters with backpropagation, are at the origin of the success of artificial intelligence. \n",
    "\n",
    "In this second part, we are going to train an artificial neural network on a classic task, namely classifying images of handwritten digits (the famous 'MNIST' dataset). Although this is not even close to showing the full potential of modern state of the art networks, it should give you an idea that we can use the principles illustrated here to do some actually cool stuff. Let's take a look at the data! Run the cell below to plot some random images from our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "f, axes = plt.subplots(2, 2)\n",
    "for ax in axes.flatten():\n",
    "    ind = np.random.choice(x_train.shape[0])\n",
    "    ax.imshow(x_train[ind], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every image in our dataset, we have an associated label telling us what digit the image corresponds to. We are going to train the network to recognize these images. Every image is 28 by 28 pixels (784 pixels in total). We will split our dataset into a part that we will use to train the network, and leave out some data to _test_ how well the network can classify digits that it hasn't seen before.\n",
    "\n",
    "To construct the network, we will use a popular high-level library called Keras, which allows us to define and train and architecture with a few lines of code. Modern neural network libraries rely on _automatic differentiation_, which means that all the gradients (like the ones we computed in the first part) are actually computed for us, so that we don't have to worry about it. The example we will use is adapted from the Keras documentation.\n",
    "\n",
    "When we instantiate the simulation, three widgets will pop up, which will allows you to specify\n",
    "- the number of hidden layers in the model\n",
    "- the number of units per layer\n",
    "- the number of training epochs\n",
    "\n",
    "An epoch is essentially a round of training on all images in the dataset. Usually, we need several epochs to get a good accuracy on a dataset (our network needs to 'see' the data a few times to properly learn it). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "We are going to start by making a network with one hidden layer with 15 units. So our input, which is 784 pixels, is going to be passed through a hidden layer of size 15, which is then connected to an output unit of size 10 (one neuron per digit in the output layer). Do you think this network is going to be able to learn how to classify digits well? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "Personal answer, can vary per individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Run the cell below, which will set up the simulation. Edit the settings to have one hidden layer with 15 units and training for 10 epochs, then press 'Initialize and train'. \n",
    "\n",
    "Once training is finished, read and try to understand the output of the training. The metric that we are most interested in is the __validation accuracy__, which measures how many images in the test set we were able to classify correctly (number of correctly classified divided by number of images in the test set - with accuracy of 1 our network can classify perfectly).\n",
    "\n",
    "How does the achieved accuracy compare to your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_assignment_4 import ffnn_simulation\n",
    "simulation = ffnn_simulation()\n",
    "simulation.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2\n",
    "Personal answer, can vary per individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Let us now run a simulation again, this time let's use two layers with 50 neurons each, and train for 10 epochs. Inspect your results, and compare them with the previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation = ffnn_simulation()\n",
    "simulation.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3\n",
    "Accuracy is improved. However, the improvement should be minimal (92-95 with single layer to 95-98 with two layers, 50 each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "The output of the simulation says `Train on 60000 samples, validate on 10000 samples`, meaning that the network was trained using 60000 samples, whereas the evaluation of the accuracy was performed using a separate set of 10000 samples. Why would you want to use separate subsets of data to train and evaluate your network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4\n",
    "\n",
    "The network can use 'spurious' features in the training set to classify this data. It will optimize the parameters so that on these particular inputs, classification accuracy is highest. It might however use information not pertaining to actual digit. \n",
    "In other words, we would like our network to be able to generalize what it has learned to other 'unseen' samples. This shows that the network truly has learned something about the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Similarly to the last question, you can see in the output that the classification accuracy on the training set tends to be higher than the test accuracy, especially during the later training epochs. The scenario where accuracy on the training set is perfect, but the accuracy on the test data is very poor, is called overfitting. What factors in the network architecture and training procedure do you think can contribute to overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 5 (see also blow)\n",
    "- **If you have more neurons and more layers you have more parameters you can tweak. Your function becomes more flexible and easy to overfit.**\n",
    "- **If you have more training epochs the network tends to start using detailed/spurious features of the training set. Performance on the test set tends to go down after too many training epochs (overfitting). **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Now, take some time to freely experiment with the number of units, the number of layers, and the number of training epochs. Try different parameter configurations to try and maximize accuracy. Then, elaborate on your results. In particular address the following points:\n",
    "\n",
    "- With which parameters did you achieve the highest accuracy?\n",
    "- Is adding more neurons per layer always beneficial? Why/why not?\n",
    "- Is adding more layers always beneficial? Why/why not?\n",
    "- Is training for more epochs always beneficial? Why/why not?\n",
    "- What is the downside of having more neurons/layers?\n",
    "\n",
    "To play around, you can either use the `initialize and train` button, or you can also create a new cell, then type the following two lines to instantiate a new simulation:\n",
    "\n",
    "`simulation = ffnn_simulation()\n",
    "simulation.start()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation = ffnn_simulation()\n",
    "simulation.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 6\n",
    "- With which parameters did you achieve the highest accuracy?\n",
    "- **~2/3 layers, 200-500 neurons. Award for network with >99% accuracy**\n",
    "- ****\n",
    "\n",
    "- Is adding more neurons always beneficial? Why/why not?\n",
    "- **See below**\n",
    "- ****\n",
    "\n",
    "- Is training for more epochs always beneficial? Why/why not?\n",
    "- **There is the risk of overfitting which will increase accuracy on the training dataset, but not on the testing dataset.**\n",
    "- ****\n",
    "\n",
    "- What is the downside of having more neurons/layers?\n",
    "- **If you have more neurons and more layers, you need more data to train. **\n",
    "- **If you have more neurons and more layers you have more parameters you can tweak. Your function becomes more flexible and easy to overfit.**\n",
    "- **It takes more time to train**\n",
    "- ****\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
